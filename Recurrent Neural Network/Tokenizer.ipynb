{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a53260-4ec9-4e27-b480-43360b338dd7",
   "metadata": {},
   "source": [
    "#### Types of Tokenization Methods\n",
    "***\n",
    "1. Word Tokenization\n",
    "2. Character Tokenization\n",
    "3. Sub-Word Tokenization\n",
    "\n",
    "More INFO: [DataCamp Tokenization Explanation](https://www.datacamp.com/blog/what-is-tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2bea9e-1bb4-4458-a237-d6fed798924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0ce30cf8-c210-4042-8777-eb58fd8d5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = set(stopwords.words('english'))\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''\n",
    "\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def text_preprocess(sen):\n",
    "    \n",
    "    sen = sen.lower()\n",
    "        \n",
    "    # Remove html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        \n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
    "        \n",
    "    # Remove Stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords_list) + r')\\b\\s*')\n",
    "    sentence = pattern.sub('', sentence)\n",
    "\n",
    "    return sentence.rstrip()\n",
    "\n",
    "# The code above ^ https://github.com/skillcate/sentiment-analysis-with-deep-neural-networks/blob/main/b2_preprocessing_function.py\n",
    "\n",
    "def n_grams(data, n:int=3):\n",
    "    n_grams_list = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i]) - n + 1):\n",
    "            n_grams_list.append(data[i][j: j + n])\n",
    "    \n",
    "    return n_grams_list\n",
    "\n",
    "def clean_data(data):\n",
    "    clean_data = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        clean_data.append((text_preprocess(sentence).split(\" \")))\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "59644bed-2b38-4a81-b339-5fca7bb459da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Data\n",
    "\n",
    "data = np.array(['Branden is good person\\n',\n",
    " 'Shlok is a great man\\n',\n",
    " 'Jason is a nice person\\n',\n",
    " 'David is a bad human\\n',\n",
    " 'Chris has a great personality\\n'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d12d5-c7ac-41fd-acb6-ef3ca5bdd9b1",
   "metadata": {},
   "source": [
    "#### 1. Word Tokenization\n",
    "***\n",
    "- Splitting the sentence into word serperate words then convert them using word_to_index method\n",
    "- Most common approach and particulary effective for languages with clear boundaries\n",
    "\n",
    "1. Benefits\n",
    "    - Simpliest way to seperate speech or text into parts\n",
    "2. Cons\n",
    "    - Difficult for word tokenization to seperate unknown words or Out of Vocavulary Words\n",
    "    - Temporary solution is to replace unknown words with common tokens, but there is no way to determine whether the unknown words are different or the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5cd4e6b3-5a38-4ff2-8de3-caf82a2a8a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['branden'],\n",
       " ['good'],\n",
       " ['person'],\n",
       " ['shlok'],\n",
       " ['great'],\n",
       " ['man'],\n",
       " ['jason'],\n",
       " ['nice'],\n",
       " ['person'],\n",
       " ['david'],\n",
       " ['bad'],\n",
       " ['human'],\n",
       " ['chris'],\n",
       " ['great'],\n",
       " ['personality'],\n",
       " ['sara'],\n",
       " ['interesting'],\n",
       " ['woman']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using n_grams to create word tokens\n",
    "n_grams(clean_data(data), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d187993-660c-4d52-aad0-0a3d3d76e1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['david', 'bad', 'human']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using ntlk's word tokenize function to create word tokens\n",
    "word_tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54975023-9708-479a-a61c-b69ba064fd06",
   "metadata": {},
   "source": [
    "#### 2. Character Tokenization\n",
    "***\n",
    "- Splitting the sentence into characters then convert them using word_to_index method\n",
    "- Allows the tokenization process to retain information about (OOV) words that word tokenization cannot\n",
    "\n",
    "1. Benefits\n",
    "    - Allows to obtain more information/context from the corpus\n",
    "2. Cons\n",
    "   - Increases dimnensionality of word vector\n",
    "   - Example: (My name) -> (\"M\", \"y\", \"n\" \"a\", \"m\", \"e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d20a1-88e0-4a5d-949d-9c64d0c54dcb",
   "metadata": {},
   "source": [
    "#### 3. Subword Tokenization\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
