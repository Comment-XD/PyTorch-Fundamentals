{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923abfc8-aa5e-4d79-9161-0f7f670cb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer libraries and bert model for sentiment analysis\n",
    "# Using n-grams to split text to n-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a53260-4ec9-4e27-b480-43360b338dd7",
   "metadata": {},
   "source": [
    "#### Types of Tokenization Methods\n",
    "***\n",
    "1. Word Tokenization\n",
    "2. Character Tokenization\n",
    "3. Sub-Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb2bea9e-1bb4-4458-a237-d6fed798924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brand\\AppData\\Local\\Temp\\ipykernel_27432\\3683524077.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ce30cf8-c210-4042-8777-eb58fd8d5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(data, n):\n",
    "    n_grams_list = []\n",
    "\n",
    "    for _, word in enumerate(data):\n",
    "        for j in range(len(word) - n + 1):\n",
    "            n_grams_list.append(word[j: j+n])\n",
    "    \n",
    "    return n_grams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59644bed-2b38-4a81-b339-5fca7bb459da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(['Branden is good person\\n',\n",
    " 'Shlok is a great man\\n',\n",
    " 'Jason is a nice person\\n',\n",
    " 'David is a bad human\\n',\n",
    " 'Chris has a great personality\\n',\n",
    " 'Sara is an interesting woman'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5cd4e6b3-5a38-4ff2-8de3-caf82a2a8a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bra', 'ran', 'and', 'nde', 'den', 'en ', 'n i', ' is', 'is ',\n",
       "       's g', ' go', 'goo', 'ood', 'od ', 'd p', ' pe', 'per', 'ers',\n",
       "       'rso', 'son', 'on\\n', 'Shl', 'hlo', 'lok', 'ok ', 'k i', ' is',\n",
       "       'is ', 's a', ' a ', 'a g', ' gr', 'gre', 'rea', 'eat', 'at ',\n",
       "       't m', ' ma', 'man', 'an\\n', 'Jas', 'aso', 'son', 'on ', 'n i',\n",
       "       ' is', 'is ', 's a', ' a ', 'a n', ' ni', 'nic', 'ice', 'ce ',\n",
       "       'e p', ' pe', 'per', 'ers', 'rso', 'son', 'on\\n', 'Dav', 'avi',\n",
       "       'vid', 'id ', 'd i', ' is', 'is ', 's a', ' a ', 'a b', ' ba',\n",
       "       'bad', 'ad ', 'd h', ' hu', 'hum', 'uma', 'man', 'an\\n', 'Chr',\n",
       "       'hri', 'ris', 'is ', 's h', ' ha', 'has', 'as ', 's a', ' a ',\n",
       "       'a g', ' gr', 'gre', 'rea', 'eat', 'at ', 't p', ' pe', 'per',\n",
       "       'ers', 'rso', 'son', 'ona', 'nal', 'ali', 'lit', 'ity', 'ty\\n',\n",
       "       'Sar', 'ara', 'ra ', 'a i', ' is', 'is ', 's a', ' an', 'an ',\n",
       "       'n i', ' in', 'int', 'nte', 'ter', 'ere', 'res', 'est', 'sti',\n",
       "       'tin', 'ing', 'ng ', 'g w', ' wo', 'wom', 'oma', 'man'],\n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_tokenizer = np.array(n_grams(data, 3))\n",
    "n_gram_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d187993-660c-4d52-aad0-0a3d3d76e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = n_grams([word_tokenize(data[0])], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "daf8e1bb-694c-4f34-8f6e-866f9d956b3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m coun_vect \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 2\u001b[0m count_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcoun_vect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m count_array \u001b[38;5;241m=\u001b[39m count_matrix\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mcount_array,columns \u001b[38;5;241m=\u001b[39m coun_vect\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32m~\\project_env\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\project_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\project_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\project_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\project_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "coun_vect = CountVectorizer()\n",
    "count_matrix = coun_vect.fit_transform(data)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = coun_vect.get_feature_names_out())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
