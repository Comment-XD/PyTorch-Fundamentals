{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb1ad40-93f1-4db5-a652-a1aad1266aa9",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "***\n",
    "- LSTM(Long Short Term Memory) is a more enhanced RNN that is used to counteract the exploding/ vanishing gradient problems caused by a vanilla RNN (Recurrent Neural Network)\n",
    "\n",
    "1. **Notebook Summary:**\n",
    "- Inner Workings Behind LSTM\n",
    "- Different Applications of LSTM\n",
    "    - Stacked LSTM\n",
    "    - Parallel LSTM\n",
    "    - Bidirectional LSTM\n",
    "- PyTorch Example of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "980ffa52-aa75-4445-8824-ef20e9ef070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "736365d2-a337-4051-a7fd-098c1d2f0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScratch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.w_ii = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "        self.w_if = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "        self.w_ig = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "        self.w_io = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "\n",
    "        self.b_ii = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_if = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_ig = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_io = nn.Parameter(torch.rand([1])).to(device)\n",
    "        \n",
    "        self.w_hi = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "        self.w_hf = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "        self.w_hg = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "        self.w_ho = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "\n",
    "        self.b_hi = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_hf = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_hg = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_ho = nn.Parameter(torch.rand([1])).to(device)\n",
    "        \n",
    "    def forward(self, x_t, prev_h, prev_c):\n",
    "        \n",
    "        \n",
    "        x_t = x_t.to(device)\n",
    "        prev_h = prev_h.to(device)\n",
    "        prev_c = prev_c.to(device)\n",
    "        \n",
    "        i_t = F.sigmoid(torch.matmul(x_t, self.w_ii) + self.b_ii + torch.matmul(self.w_hi, prev_h) + self.b_hi)\n",
    "        f_t = F.sigmoid(torch.matmul(x_t, self.w_if) + self.b_if + torch.matmul(self.w_hf, prev_h) + self.b_hf)\n",
    "        g_t = F.tanh(torch.matmul(x_t, self.w_ig) + self.b_ig + torch.matmul(self.w_hg, prev_h) + self.b_hg)\n",
    "        o_t = F.tanh(torch.matmul(x_t, self.w_io) + self.b_io + torch.matmul(self.w_ho, prev_h) + self.b_ho)\n",
    "        c_t = torch.multiply(f_t, prev_c) + torch.multiply(i_t, g_t)\n",
    "        h_t = torch.multiply(o_t, F.tanh(c_t))\n",
    "\n",
    "        return h_t, h_t, c_t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fd7f43f1-ee2f-40b2-b7f0-49b49a470a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9901]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[0.9901]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[1.0629e+37]], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_scratch = LSTMScratch(2, 1)\n",
    "lstm_scratch.forward(torch.Tensor([[1,2]]), torch.FloatTensor(1), torch.FloatTensor(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd159b14-922e-4d2c-b389-087ef84a8f5f",
   "metadata": {},
   "source": [
    "### 3. Pytorch `nn.RNN(input_size, hidden_state, num_layers)`\n",
    "***\n",
    "- `input_size`: the number of expect feature in the input x\n",
    "- `hidden_size`: the number of features in the hidden state\n",
    "- `num_layers`: the number of recurrent layers \n",
    "    <br></br>\n",
    "1. `.forward(inputs, hidden_state=torch.zeros(1, hidden_state)))`\n",
    "     - forward has two inputs, the previous activation layer and the input values\n",
    "     - for the previous activation layer, when set to a dimension, create a zero vector of that dimension as its default value\n",
    "       - inputs: represents the data you want to look through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fe448d51-be8a-4c3d-9910-55c6cdeb34aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],\n",
       "        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],\n",
       "        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "data = torch.rand(3, 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0cccaf26-9117-4ef6-9ef8-e6594ebbcdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1597],\n",
       "         [-0.1880],\n",
       "         [-0.2430]], grad_fn=<SqueezeBackward1>),\n",
       " (tensor([[-0.2430]], grad_fn=<SqueezeBackward1>),\n",
       "  tensor([[-0.2480]], grad_fn=<SqueezeBackward1>)))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(2, 1)\n",
    "lstm.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe065fd0-a53b-480e-b19e-206b751eeb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(3))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a77f1aef-7719-4719-abbf-e3d1cd9706a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_29 (LSTM)              (None, 4)                 240       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245 (980.00 Byte)\n",
      "Trainable params: 245 (980.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# embedding_layer = keras.layers.Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "\n",
    "# snn_model.add(embedding_layer)\n",
    "model.add(keras.Input((1,10)))\n",
    "model.add(keras.layers.LSTM(4)) # Determines the Number of LSTM Cells\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71abaec8-d544-4bb9-9e54-1515c966e826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.tanh(torch.Tensor([-10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
