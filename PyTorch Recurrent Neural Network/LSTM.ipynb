{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb1ad40-93f1-4db5-a652-a1aad1266aa9",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "***\n",
    "- LSTM(Long Short Term Memory) is a more enhanced RNN that is used to counteract the exploding/ vanishing gradient problems caused by a vanilla RNN (Recurrent Neural Network)\n",
    "\n",
    "1. **Notebook Summary:**\n",
    "- Inner Workings Behind LSTM\n",
    "- Different Applications of LSTM\n",
    "    - Stacked LSTM\n",
    "    - Parallel LSTM\n",
    "    - Bidirectional LSTM\n",
    "- PyTorch Example of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980ffa52-aa75-4445-8824-ef20e9ef070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736365d2-a337-4051-a7fd-098c1d2f0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScratch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for the Inputs\n",
    "        self.w_ii = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "        self.w_if = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "        self.w_ig = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "        self.w_io = nn.Parameter(torch.rand(input_size, hidden_size)).to(device)\n",
    "\n",
    "        # Biases for the Inputs\n",
    "        self.b_ii = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_if = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_ig = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_io = nn.Parameter(torch.rand([1])).to(device)\n",
    "\n",
    "        # Weights for the Hidden State\n",
    "        self.w_hi = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "        self.w_hf = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "        self.w_hg = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "        self.w_ho = nn.Parameter(torch.rand(hidden_size, hidden_size)).to(device)\n",
    "\n",
    "        # Biases for the Hidden State\n",
    "        self.b_hi = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_hf = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_hg = nn.Parameter(torch.rand([1])).to(device)\n",
    "        self.b_ho = nn.Parameter(torch.rand([1])).to(device)\n",
    "        \n",
    "    def forward(self, x_t, prev_h, prev_c):\n",
    "        \n",
    "        \n",
    "        x_t = x_t.to(device)\n",
    "        prev_h = prev_h.to(device)\n",
    "        prev_c = prev_c.to(device)\n",
    "        \n",
    "        i_t = F.sigmoid(torch.matmul(x_t, self.w_ii) + self.b_ii + torch.matmul(self.w_hi, prev_h) + self.b_hi)\n",
    "        f_t = F.sigmoid(torch.matmul(x_t, self.w_if) + self.b_if + torch.matmul(self.w_hf, prev_h) + self.b_hf)\n",
    "        g_t = F.tanh(torch.matmul(x_t, self.w_ig) + self.b_ig + torch.matmul(self.w_hg, prev_h) + self.b_hg)\n",
    "        o_t = F.tanh(torch.matmul(x_t, self.w_io) + self.b_io + torch.matmul(self.w_ho, prev_h) + self.b_ho)\n",
    "        c_t = torch.multiply(f_t, prev_c) + torch.multiply(i_t, g_t)\n",
    "        h_t = torch.multiply(o_t, F.tanh(c_t))\n",
    "\n",
    "        return h_t, c_t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7f43f1-ee2f-40b2-b7f0-49b49a470a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5858]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[0.6742]], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_scratch = LSTMScratch(2, 1)\n",
    "lstm_scratch.forward(torch.Tensor([[1,2]]), torch.FloatTensor(1), torch.FloatTensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a180e1e-3fc9-488f-aded-f9839e1d30f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0988]], grad_fn=<SqueezeBackward1>),\n",
       " [tensor([[0.1508]], grad_fn=<SqueezeBackward1>),\n",
       "  tensor([[-0.0988]], grad_fn=<SqueezeBackward1>)],\n",
       " [(tensor([[0.1508]], grad_fn=<SqueezeBackward1>),\n",
       "   tensor([[0.4671]], grad_fn=<SqueezeBackward1>)),\n",
       "  (tensor([[-0.0988]], grad_fn=<SqueezeBackward1>),\n",
       "   tensor([[-0.2313]], grad_fn=<SqueezeBackward1>))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StackedLSTMScratch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers:int=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size)\n",
    "        self.lstm2 = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_state_list = []\n",
    "        cell_state_list = []\n",
    "        h_t0, c_t0 = self.lstm1(x)\n",
    "        hidden_state_list.append(h_t0)\n",
    "        cell_state_list.append(c_t0)\n",
    "        \n",
    "        h_t1, c_t1 = self.lstm2(h_t0)\n",
    "        \n",
    "        hidden_state_list.append(h_t1)\n",
    "        cell_state_list.append(c_t1)\n",
    "\n",
    "        return h_t1, hidden_state_list, cell_state_list\n",
    "        \n",
    "        \n",
    "\n",
    "stackLSTM = StackedLSTMScratch(2,1,2)\n",
    "stackLSTM.forward(torch.Tensor([[1,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd159b14-922e-4d2c-b389-087ef84a8f5f",
   "metadata": {},
   "source": [
    "### 3. PyTorch \n",
    "***\n",
    "\n",
    "**`nn.RNN(input_size, hidden_state, num_layers)`**\n",
    "- `input_size`: the number of expect feature in the input x\n",
    "- `hidden_size`: the number of features in the hidden state\n",
    "- `num_layers`: the number of RNNs stacked together\n",
    "    <br></br>\n",
    "1. `.forward(inputs, hidden_state=torch.zeros(1, hidden_state)))`\n",
    "     - forward has two inputs, the previous activation layer and the input values\n",
    "     - for the previous activation layer, when set to a dimension, create a zero vector of that dimension as its default value\n",
    "       - inputs: represents the data you want to look through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe448d51-be8a-4c3d-9910-55c6cdeb34aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "data = torch.rand(1, 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cccaf26-9117-4ef6-9ef8-e6594ebbcdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2389]], grad_fn=<SqueezeBackward1>),\n",
       " (tensor([[ 0.1356],\n",
       "          [ 0.2857],\n",
       "          [-0.0390],\n",
       "          [-0.0674],\n",
       "          [ 0.0053],\n",
       "          [ 0.0398],\n",
       "          [ 0.0340],\n",
       "          [-0.0032],\n",
       "          [ 0.1631],\n",
       "          [ 0.2389]], grad_fn=<SqueezeBackward1>),\n",
       "  tensor([[ 0.2596],\n",
       "          [ 0.4242],\n",
       "          [-0.0732],\n",
       "          [-0.2216],\n",
       "          [ 0.0178],\n",
       "          [ 0.0833],\n",
       "          [ 0.2536],\n",
       "          [-0.0051],\n",
       "          [ 0.4905],\n",
       "          [ 0.5370]], grad_fn=<SqueezeBackward1>)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(2, 1, num_layers=10) # Num of layers determines the number of units \n",
    "\n",
    "lstm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe065fd0-a53b-480e-b19e-206b751eeb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(3))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a77f1aef-7719-4719-abbf-e3d1cd9706a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_29 (LSTM)              (None, 4)                 240       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245 (980.00 Byte)\n",
      "Trainable params: 245 (980.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# embedding_layer = keras.layers.Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "\n",
    "# snn_model.add(embedding_layer)\n",
    "model.add(keras.Input((1,10)))\n",
    "model.add(keras.layers.LSTM(4)) # Determines the Number of LSTM Cells\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71abaec8-d544-4bb9-9e54-1515c966e826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.tanh(torch.Tensor([-10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
