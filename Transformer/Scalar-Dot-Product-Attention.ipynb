{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709c106c-f9e7-48a6-af59-835c88dcd7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brand\\AppData\\Local\\Temp\\ipykernel_2164\\946988000.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (transformer.py, line 64)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\project_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 9\u001b[1;36m\n\u001b[1;33m    from transformer import Positional_Encoding\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\project_env\\PyTorch-Fundamentals\\Transformer\\transformer.py:64\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, d_model:int, heads:int:4, dropout_value:int=0.1, mask=None) -> None:\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "from transformer import Positional_Encoding\n",
    "\n",
    "torch.set_printoptions(precision=3) # Sets the precision of torch tensors to the thousands place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1a4da9-87c3-4a2c-8395-26ac73e18a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8 # the length of the embedding dimension\n",
    "max_length = 3000 # the total indexes we are producing for out positional encodings\n",
    "\n",
    "data = torch.rand(20, 6).long() # Creating our data (20 sentences with 6 words in each sentence)\n",
    "# [[w_11, w_12, ... w_1n]]\n",
    "# [[w_21, ..., ...  ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[w_m1, ..., ...  w_mn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f642b37-f6ab-4f84-a8b3-70bc9aee2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming our vocab size = 40\n",
    "# We create embedding dimensions of d_model, which in this case = 8\n",
    "embedding_layer = nn.Embedding(40, embedding_dim=d_model)\n",
    "embeddings = embedding_layer(data)\n",
    "\n",
    "# create positional encodings = to the embedding dimensions (which is 8)\n",
    "positional_layer = Positional_Encoding(d_model=d_model, max_length=max_length)\n",
    "\n",
    "# Add upon our word embeddings to our positional_encodings\n",
    "positional_encodings = positional_layer(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17d2fdd-240a-4ed4-9ccb-e561f91519fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar-Dot-Product Attention\n",
    "\n",
    "# Create FeedForward Layer for Query, Key, Value weights\n",
    "# The weights' dimensions all need to be the same dimensions (8x8)\n",
    "\n",
    "query_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "key_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "value_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Implement Broadcasting Matrix Multiplication\n",
    "# Should return the same dimensions for Q, K, V\n",
    "\n",
    "Q = query_weights(positional_encodings)\n",
    "K = key_weights(positional_encodings)\n",
    "V = value_weights(positional_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d321eb9f-9e3f-4f04-a376-c52fe8a7e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimension: torch.Size([20, 6, 8])\n",
      "Reshaped Dimension: torch.Size([20, 6, 2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 6, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query, value, key matrix should all be the same size\n",
    "batch_size = Q.size(0)\n",
    "\n",
    "num_heads = 2\n",
    "d_keys = d_model // num_heads\n",
    "\n",
    "# view is essentially reshape for pytorch\n",
    "# heads in multi-head attention essentially act like workers as they divide up the embeddings into smaller groups \n",
    "# this allows faster performace \n",
    "# - 1 means length of the dimension\n",
    "\n",
    "# Original Dimensions: [Batch_Size, Sentence_Length, Embedding_Dimensions (d_model)] \n",
    "print(f\"Original Dimension: {Q.size()}\")\n",
    "Q = Q.view(batch_size, -1, num_heads, d_keys)\n",
    "K = K.view(batch_size, -1, num_heads, d_keys)\n",
    "V = V.view(batch_size, -1, num_heads, d_keys)\n",
    "\n",
    "# Reshaped Dimensions: [Batch_Size, Sentence_Length, Num_Of_Heads, Embedding_Dimensions (d_model) / Num_Of_Heads]\n",
    "print(f\"Reshaped Dimension: {Q.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b28398e-858a-4e9b-9903-f1c1a1f5456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Dimension: torch.Size([20, 6, 2, 4])\n",
      "Permuted Dimension: torch.Size([20, 2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Reording the Reshaped Dimension to be [Batch_Size, Num_Of_Heads, Sentence_Length, Embedding_Dimensions (d_model) / Num_Of_Heads]\n",
    "print(f\"Reshaped Dimension: {Q.size()}\")\n",
    "\n",
    "Q = Q.permute(0,2,1,3)\n",
    "K = K.permute(0,2,1,3)\n",
    "V = V.permute(0,2,1,3)\n",
    "\n",
    "print(f\"Permuted Dimension: {Q.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "629ea713-b481-4a2e-b2c7-6fdfe59517f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2, 6, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_T = K.permute(0,1,3,2)\n",
    "\n",
    "scaled_dot_prod = (Q @ K_T) / math.sqrt(d_keys)\n",
    "attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "attention_scores = attention_probs @ V\n",
    "\n",
    "attention_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ab04490-8812-4006-94a1-f732457373a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar_Dot_Product_Attention(nn.Module):\n",
    "    def __init__(self, d_model:int, mask=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        d_model (int): the dimension of the word embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Query, Key, Value Weights\n",
    "        self.Qw = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Kw = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Vw = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, X:torch.Tensor()) -> torch.Tensor():\n",
    "        \"\"\"\n",
    "        X (torch.Tensor): a Tensor that contains the sum between the Word Embeddings and Positional Encodings\n",
    "\n",
    "        returns (torch.Tensor): Returns the attention score of the input X\n",
    "        \"\"\"\n",
    "        Q = X @ self.Qw\n",
    "        K = X @ self.Kw\n",
    "        V = X @ self.Vw\n",
    "\n",
    "        scaled_dot_prod = (Q @ K.permute(0,2,1)) / math.sqrt(d_model)\n",
    "        attention_prob = scaled_dot_prod.softmax(dim=-1)\n",
    "\n",
    "        attention_scores = attention_prob @ V\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7b14e3-cb74-4c97-9809-fecd93d31eeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScalar_Product_Attention\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model:\u001b[38;5;28mint\u001b[39m, normalized: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Scalar_Product_Attention(nn.Module):\n",
    "    def __init__(self, d_model:int, normalized: bool=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_weights = nn.Linear(d_model, d_model)\n",
    "        self.value_weights = nn.Linear(d_model, d_model)\n",
    "        self.key_weights = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.normalized = normalized\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        query_matrix = x @ self.query_weights\n",
    "        value_matrix = x @ self.value_weights\n",
    "        key_matrix =  x @ self.key_weights\n",
    "        \n",
    "        dot_product = query_matrix @ key_matrix.permute(0,2,1)\n",
    "\n",
    "        if self.normalized:\n",
    "            dot_product = torch.divide(dot_product, torch.sqrt(key_matrix.size))\n",
    "                \n",
    "        softmax_dot_product = torch.nn.functional.softmax(dot_product, dim=-1)\n",
    "\n",
    "        attention_score = softmax_dot_product @ value_matrix\n",
    "\n",
    "        return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e60c8774-b57f-44bd-b5d3-dc477cb9bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_product_attention = Scalar_Product_Attention((2,2), normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39e8b2b9-e480-4664-bee9-b3a7c6b5fcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.9998, 0.6929],\n",
       "        [0.8881, 0.5199]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_product_attention.query_weights\n",
    "scalar_product_attention.value_weights\n",
    "scalar_product_attention.key_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9adf5ac6-a68e-4b72-97ef-b4cff09c2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_product_attention.query_weights.size()\n",
    "scalar_product_attention.value_weights.size()\n",
    "scalar_product_attention.key_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0171e8fb-2b3d-4393-9349-baa88d05bd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7761, 0.6186],\n",
       "        [0.7742, 0.6172]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_product_attention.forward(torch.rand(2,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
