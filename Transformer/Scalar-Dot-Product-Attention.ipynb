{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709c106c-f9e7-48a6-af59-835c88dcd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "from transformer import Positional_Encoding\n",
    "\n",
    "torch.set_printoptions(precision=3) # Sets the precision of torch tensors to the thousands place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1a4da9-87c3-4a2c-8395-26ac73e18a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8 # the length of the embedding dimension\n",
    "max_length = 3000 # the total indexes we are producing for out positional encodings\n",
    "\n",
    "data = torch.rand(20, 6).long() # Creating our data (20 sentences with 6 words in each sentence)\n",
    "# [[w_11, w_12, ... w_1n]]\n",
    "# [[w_21, ..., ...  ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[w_m1, ..., ...  w_mn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f642b37-f6ab-4f84-a8b3-70bc9aee2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming our vocab size = 40\n",
    "# We create embedding dimensions of d_model, which in this case = 8\n",
    "embedding_layer = nn.Embedding(40, embedding_dim=d_model)\n",
    "embeddings = embedding_layer(data)\n",
    "\n",
    "# create positional encodings = to the embedding dimensions (which is 8)\n",
    "positional_layer = Positional_Encoding(d_model=d_model, max_length=max_length)\n",
    "\n",
    "# Add upon our word embeddings to our positional_encodings\n",
    "positional_encodings = positional_layer(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17d2fdd-240a-4ed4-9ccb-e561f91519fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar-Dot-Product Attention\n",
    "\n",
    "# Create FeedForward Layer for Query, Key, Value weights\n",
    "# The weights' dimensions all need to be the same dimensions (8x8)\n",
    "\n",
    "query_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "key_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "value_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Implement Broadcasting Matrix Multiplication\n",
    "# Should return the same dimensions for Q, K, V\n",
    "\n",
    "Q = query_weights(positional_encodings)\n",
    "K = key_weights(positional_encodings)\n",
    "V = value_weights(positional_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d321eb9f-9e3f-4f04-a376-c52fe8a7e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimension: torch.Size([20, 6, 8])\n",
      "Reshaped Dimension: torch.Size([20, 6, 2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 6, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query, value, key matrix should all be the same size\n",
    "batch_size = Q.size(0)\n",
    "\n",
    "num_heads = 2\n",
    "d_keys = d_model // num_heads\n",
    "\n",
    "# view is essentially reshape for pytorch\n",
    "# heads in multi-head attention essentially act like workers as they divide up the embeddings into smaller groups \n",
    "# this allows faster performace \n",
    "# - 1 means length of the dimension\n",
    "\n",
    "# Original Dimensions: [Batch_Size, Sentence_Length, Embedding_Dimensions (d_model)] \n",
    "print(f\"Original Dimension: {Q.size()}\")\n",
    "Q = Q.view(batch_size, -1, num_heads, d_keys)\n",
    "K = K.view(batch_size, -1, num_heads, d_keys)\n",
    "V = V.view(batch_size, -1, num_heads, d_keys)\n",
    "\n",
    "# Reshaped Dimensions: [Batch_Size, Sentence_Length, Num_Of_Heads, Embedding_Dimensions (d_model) / Num_Of_Heads]\n",
    "print(f\"Reshaped Dimension: {Q.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b28398e-858a-4e9b-9903-f1c1a1f5456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Dimension: torch.Size([20, 6, 2, 4])\n",
      "Permuted Dimension: torch.Size([20, 2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Reording the Reshaped Dimension to be [Batch_Size, Num_Of_Heads, Sentence_Length, Embedding_Dimensions (d_model) / Num_Of_Heads]\n",
    "print(f\"Reshaped Dimension: {Q.size()}\")\n",
    "\n",
    "Q = Q.permute(0,2,1,3)\n",
    "K = K.permute(0,2,1,3)\n",
    "V = V.permute(0,2,1,3)\n",
    "\n",
    "print(f\"Permuted Dimension: {Q.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "629ea713-b481-4a2e-b2c7-6fdfe59517f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2, 6, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_T = K.permute(0,1,3,2)\n",
    "\n",
    "scaled_dot_prod = (Q @ K_T) / math.sqrt(d_keys)\n",
    "attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "attention_scores = attention_probs @ V\n",
    "\n",
    "attention_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ab04490-8812-4006-94a1-f732457373a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar_Dot_Product_Attention(nn.Module):\n",
    "    def __init__(self, d_model:int, mask=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        d_model (int): the dimension of the word embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Query, Key, Value Weights\n",
    "        self.Qw = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Kw = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Vw = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, X:torch.Tensor()) -> torch.Tensor():\n",
    "        \"\"\"\n",
    "        X (torch.Tensor): a Tensor that contains the sum between the Word Embeddings and Positional Encodings\n",
    "\n",
    "        returns (torch.Tensor): Returns the attention score of the input X\n",
    "        \"\"\"\n",
    "        Q = X @ self.Qw\n",
    "        K = X @ self.Kw\n",
    "        V = X @ self.Vw\n",
    "\n",
    "        scaled_dot_prod = (Q @ K.permute(0,2,1)) / math.sqrt(d_model)\n",
    "        attention_prob = scaled_dot_prod.softmax(dim=-1)\n",
    "\n",
    "        attention_scores = attention_prob @ V\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b7b14e3-cb74-4c97-9809-fecd93d31eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar_Product_Attention(nn.Module):\n",
    "    def __init__(self, shape:tuple, normalized: bool=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_weights = nn.Parameter(torch.rand(shape))\n",
    "        self.value_weights = nn.Parameter(torch.rand(shape))\n",
    "        self.key_weights = nn.Parameter(torch.rand(shape))\n",
    "\n",
    "        self.normalized = normalized\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        query_matrix = x @ self.query_weights\n",
    "        value_matrix = x @ self.value_weights\n",
    "        key_matrix =  x @ self.key_weights\n",
    "        \n",
    "        dot_product = query_matrix @ key_matrix.T\n",
    "\n",
    "        if self.normalized:\n",
    "            dot_product = torch.divide(dot_product, torch.sqrt(key_matrix.size))\n",
    "                \n",
    "        softmax_dot_product = torch.nn.functional.softmax(dot_product, dim=1)\n",
    "\n",
    "        attention_score = softmax_dot_product @ value_matrix\n",
    "\n",
    "        return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e60c8774-b57f-44bd-b5d3-dc477cb9bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_product_attention = Scalar_Product_Attention((2,2), normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39e8b2b9-e480-4664-bee9-b3a7c6b5fcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.9998, 0.6929],\n",
       "        [0.8881, 0.5199]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_product_attention.query_weights\n",
    "scalar_product_attention.value_weights\n",
    "scalar_product_attention.key_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9adf5ac6-a68e-4b72-97ef-b4cff09c2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_product_attention.query_weights.size()\n",
    "scalar_product_attention.value_weights.size()\n",
    "scalar_product_attention.key_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0171e8fb-2b3d-4393-9349-baa88d05bd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7761, 0.6186],\n",
       "        [0.7742, 0.6172]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_product_attention.forward(torch.rand(2,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
