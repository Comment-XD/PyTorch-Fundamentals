{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1796a9d0-84e3-475e-8640-14f885da2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "from transformer import Positional_Encoding\n",
    "import math\n",
    "\n",
    "torch.set_printoptions(precision=3) # Sets the precision of torch tensors to the thousands place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0142ba-bc98-4888-b624-db1313b92b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8 # the length of the embedding dimension\n",
    "max_length = 3000 # the total indexes we are producing for out positional encodings\n",
    "\n",
    "data = torch.rand(20, 6).long() # Creating our data (20 sentences with 6 words in each sentence)\n",
    "# [[w_11, w_12, ... w_1n]]\n",
    "# [[w_21, ..., ...  ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[..., ..., ...   ... ]]\n",
    "# [[w_m1, ..., ...  w_mn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc67cff-1e39-4808-9265-b263d515ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming our vocab size = 40\n",
    "# We create embedding dimensions of d_model, which in this case = 8\n",
    "embedding_layer = nn.Embedding(40, embedding_dim=d_model)\n",
    "embeddings = embedding_layer(data)\n",
    "\n",
    "# create positional encodings = to the embedding dimensions (which is 8)\n",
    "positional_layer = Positional_Encoding(d_model=d_model, max_length=max_length)\n",
    "\n",
    "# Add upon our word embeddings to our positional_encodings\n",
    "positional_encodings = positional_layer(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4835c4d-baec-4777-bd92-cfba8e18bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar-Dot-Product Attention\n",
    "\n",
    "# Create FeedForward Layer for Query, Key, Value weights\n",
    "# The weights' dimensions all need to be the same dimensions (8x8)\n",
    "\n",
    "query_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "key_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "value_weights = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Implement Broadcasting Matrix Multiplication\n",
    "# Should return the same dimensions for Q, K, V\n",
    "\n",
    "Q = query_weights(positional_encodings)\n",
    "K = key_weights(positional_encodings)\n",
    "V = value_weights(positional_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "441999d8-c3a9-4e66-b322-62fe4e192173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimension: torch.Size([20, 6, 8])\n",
      "Reshaped Dimension: torch.Size([20, 6, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# The query, value, key matrix should all be the same size\n",
    "batch_size = Q.size(0)\n",
    "\n",
    "num_heads = 2\n",
    "d_keys = d_model // num_heads\n",
    "\n",
    "# view is essentially reshape for pytorch\n",
    "# heads in Multi-Head Attention essentially act like workers as they divide up the embeddings into smaller groups \n",
    "# this allows faster performace \n",
    "# - 1 means length of the dimension\n",
    "\n",
    "# Original Dimensions: [Batch_Size, Sentence_Length, Embedding_Dimensions (d_model)] \n",
    "print(f\"Original Dimension: {Q.size()}\")\n",
    "Q = Q.view(batch_size, -1, num_heads, d_keys)\n",
    "K = K.view(batch_size, -1, num_heads, d_keys)\n",
    "V = V.view(batch_size, -1, num_heads, d_keys)\n",
    "\n",
    "# Reshaped Dimensions: [Batch_Size, Sentence_Length, Num_Of_Heads, Embedding_Dimensions (d_model) / Num_Of_Heads]\n",
    "print(f\"Reshaped Dimension: {Q.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4da5449-0a7d-4cc1-bd02-96b9c5b627e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4041656035.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, d_model:int, heads:int:4, dropout_value:int=0.1, mask=None) -> None:\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, d_model:int, heads:int:4, dropout_value:int=0.1, mask=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        d_model (int): the dimension of the word embeddings\n",
    "        heads (int): the number of heads \n",
    "        \"\"\"\n",
    "\n",
    "        # Dimension of Embedding\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Number of Heads in our Multi-Head-Attention\n",
    "        self.heads = heads\n",
    "\n",
    "        # Query, Key, Value Weights\n",
    "        self.Qw = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Kw = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Vw = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Ow = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # \n",
    "        self.drop_out = nn.Dropout(p=dropout_value)\n",
    "\n",
    "    def forward(self, X:torch.Tensor()) -> torch.Tensor():\n",
    "        \"\"\"\n",
    "        X (torch.Tensor): a Tensor that contains the sum between the Word Embeddings and Positional Encodings\n",
    "\n",
    "        Returns (torch.Tensor): Returns the attention score of the input X\n",
    "        \"\"\"\n",
    "\n",
    "        # Query, Key, Value Matrix\n",
    "        Q = X @ self.Qw\n",
    "        K = X @ self.Kw\n",
    "        V = X @ self.Vw\n",
    "\n",
    "        # Batch_Size should be the same for Query, Key, and Value Matrix\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # Creates the key dimensions\n",
    "        d_keys = d_model // self.heads\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.heads, d_keys).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.heads, d_keys).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.heads, d_keys).permute(0,2,1,3)\n",
    "\n",
    "        scaled_dot_prod = (Q @ K.permute(0,2,1)) / math.sqrt(d_keys)\n",
    "        attention_prob = scaled_dot_prod.softmax(dim=-1)\n",
    "\n",
    "        \n",
    "        A = self.drop_out(attention_prob @ V)\n",
    "        A = A.permute(0,2,1,3).view(batch_size, -1, self.heads * d_keys)\n",
    "        \n",
    "        attention_scores = self.Ow(A)\n",
    "        \n",
    "        return attention_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
